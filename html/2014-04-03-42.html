<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style type="text/css">
body { font-size: 16px; }
.cal_brief { text-align: center; }
.cal_brief td:first-child { background: inherit; }
.cal_brief td { background: #ccc; width: 5ex; padding: 2px; }
.cal_big { text-align: center; padding: 0; margin: 0; }
.cal_big td { padding: 0 2px; }
.cal_mon { text-align: center; }
.cal_mon th { font-size: small; padding: 0; margin: 0; }
.cal_mon td { background: #ccc; width: 5ex; height: 1.5em;
	padding: 2px; text-align: right; }
.cal_mon td[colspan] { background: inherit; }
.cal_mon sup { color: #F0F0F0; text-align: left; float: left;
	margin-top: -2pt; font-weight: bold; }
.cal_mon a { text-align: right; margin-left: -4em; float: right; }
</style>

<title>phc-discussions - Re: [PHC] babbling about trends</title>


</head>

<BODY bgcolor="#E0E0E0" text="black" link="blue" alink="red" vlink="navy">



<TABLE bgcolor="white" width="100%" border="0" cellspacing="0" cellpadding="0">
<TR>
<TD width="39%">
<A HREF="http://lists.openwall.net">lists.openwall.net</A>
<TD width="1%" rowspan="3">&nbsp;
<TD width="60%" align="right" rowspan="3">
<A HREF="/">lists</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/announce/">announce</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-users/">owl-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-dev/">owl-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-users/">john-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-dev/">john-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwdqc-users/">passwdqc-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/yescrypt/">yescrypt</A>&nbsp;
<A HREF="http://www.openwall.com/lists/popa3d-users/">popa3d-users</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/oss-security/">oss-security</A>&nbsp;
<A HREF="http://www.openwall.com/lists/kernel-hardening/">kernel-hardening</A>&nbsp;
<A HREF="http://www.openwall.com/lists/musl/">musl</A>&nbsp;
<A HREF="http://www.openwall.com/lists/sabotage/">sabotage</A>&nbsp;
<A HREF="http://www.openwall.com/lists/tlsify/">tlsify</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwords/">passwords</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/crypt-dev/">crypt-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/xvendor/">xvendor</A>&nbsp;
/&nbsp;
<A HREF="/bugtraq/">Bugtraq</A>&nbsp;
<A HREF="/full-disclosure/">Full-Disclosure</A>&nbsp;
<A HREF="/linux-kernel/">linux-kernel</A>&nbsp;
linux-<A HREF="/netdev/">netdev</A>&nbsp;
<A HREF="/linux-ext4/">linux-ext4</A>&nbsp;
<a href="/linux-hardening/">linux-hardening</a>&nbsp;
<a href="/linux-cve-announce/">linux-cve-announce</a>&nbsp;
<a href="/phc-discussions/">PHC</a>&nbsp;
<TR><TD>
<DIV><FONT SIZE="-2"><I>Open Source and information security mailing list archives</I></FONT></DIV>
<TR><TD>&nbsp;
</TABLE>

<TABLE bgcolor="#B4D0DC" width="100%" border="0" cellspacing="0" cellpadding="1">
<TR><TD>
<TABLE width="100%" border="0" cellspacing="0" cellpadding="2">
<TR><TD bgcolor="#ECF8FF">

<a href="https://hashsuite.openwall.net/android">
Hash Suite for Android: free password hash cracker in your pocket</a>


</TABLE>
</TABLE>


<a href="41">[&lt;prev]</a> <a href="43">[next&gt;]</a> <a href="41">[&lt;thread-prev]</a> <a href="43">[thread-next&gt;]</a> <a href=".">[day]</a> <a href="..">[month]</a> <a href="../..">[year]</a> <a href="../../..">[list]</a>
<pre style="white-space: pre-wrap">
Message-ID: &lt;CAOLP8p7u8nXCahm6fEJm=Cdf7t5r1OQoCPd59HXS6489iWwgbg&#64;mail.gmail.com&gt;
Date: Thu, 3 Apr 2014 13:50:03 -0400
From: Bill Cox &lt;waywardgeek&#64;...il.com&gt;
To: discussions&#64;...sword-hashing.net
Subject: Re: [PHC] babbling about trends

On Thu, Apr 3, 2014 at 1:05 PM, Krisztián Pintér &lt;pinterkr&#64;...il.com&gt; wrote:
&gt;
&gt; for some time we have a lot of talk around here about like GPU, cache
&gt; lines, etc. this got me thinking. maybe we focus too much on today, as
&gt; opposed to tomorrow?
&gt; 2a. memory
&gt;
&gt; memory will no doubt be cheaper and cheaper. the question is whether
&gt; we will use more RAM or will it level out at 16G or 64G or something.
&gt; more precisely, what will be the price of RAM in a typical computer 10
&gt; years from now? because if it drops significantly, we lose edge. if a
&gt; computer of 2025 will have 64G RAM, but it will cost 1/100th of
&gt; today's price of 8G, attackers' cost drop significantly.

Interesting points.  I read this article in EETimes last week that
argues that Intel's FinFets are not the way to go:

<a href="http://www.eetimes.com/author.asp?section_id=36&amp;doc_id=1321674" rel="nofollow">http://www.eetimes.com/author.asp?section_id=36&amp;doc_id=1321674</a>

The argument is that the cost per gate, at least for FinFets, is
increasing at each new process node, rather than decreasing, and that
this trend will continue.  That basically means Intel CPUs are built
with technology that has already reached the end of Moore's Law, which
argues that the *cost* for a given function (specifically RAM) would
drop over time exponentially.  Integration densities are continuing
for a few more generations, but our high-end CPUs may go up in cost
over time.

He also argues that plain old planar fets have a couple more
generations where the cost per gate will continue to decrease,
indicating RAM has not yet hit the wall.  However, without some new
innovation, RAM stops getting cheaper per bit in just a few years.

&gt; 2b. architectures
&gt;
&gt; this is really a mystery. my prediction is: single core performance
&gt; will not increase that much, but parallelism will explode. i also
&gt; envision the merger of CPU and GPU, and larger control over the low
&gt; level parallelism (GPU-like approach wins). as well as increase in
&gt; register space. at this point, regular crypto will be done entirely
&gt; within the CPU, using no RAM at all. long term keys will be kept in
&gt; registers, etc. i also forecast disappearing significance of cache, as
&gt; regular memory access times will catch up.

I think most people would agree that performance per core has already
hit a wall, and that with increasing integration, we'll have even more
cores, and bigger GPUs integrated into CPUs.  However, RAM latency has
also hit a wall for similar reasons.  As we have more integration, we
can have wider and higher performance interfaces to RAM, so bandwidth
will continue to increase, but the time cost for a cache miss will
flatten out.  It seems to me that we will be sticking with our L1 and
L2 caches long term, and L3 has always been a design choice.

I think the entries that hammer cache at high speed, and which can
increase in size as the cache level they target increases, will do OK
on long-term runtime hardening.  There doesn't seem to be much
increase in L1 cache size anymore, but it's bandwidth keeps
increasing, and the largest cache (either L2 or L3) still increases
with process node.  Cache bandwidth based runtime hardening does seem
to require unpredictable reads to defend against custom ASICs, but not
GPUs.  The Bcrypt-style entries fitting in L1/L2 should do OK, though
many of these cores can be integrated on an ASIC, and that integration
level is still increasing.  The ones doing unpredictable small reads
at high bandwidth, taking up a lot of the largest cache should do well
long term, IMO.

Algorithms dependent on external cache bandwidth limits seem
susceptible to government-scale attackers who are about the only guys
with enough cash to integrate the hashing cores directly onto the
latest DRAM chips.  This would mostly eliminate the bandwidth
limitation, for hashes that fit into one DRAM chip.

&gt; conclusion
&gt;
&gt; i think we need to consider such long-term arguments when judging
&gt; proposals. we need to understand that "exhausting L2 cache this that"
&gt; kind of arguments does not hold up very well in a potential future
&gt; with no such thing as an L2 cache. they also does not hold up very
&gt; well in existing systems with no L2 cache. i'm not saying these
&gt; arguments are pointless, but their scope is limited, and this
&gt; limitedness is to be considered.

I disagree that L2 cache is going anywhere.  Unless we have a major
technology shift, I don't see it.  CPUs, more and more over time, are
just small blobs on chips that contain more and more cache RAM.  Small
unpredictable L2 cache reads at high access rates should remain
strong, given current tech trends.

Bill
</pre>
<p><a href="https://www.openwall.com/blists/">Powered by blists</a> - <a href="https://lists.openwall.net">more mailing lists</a>


<p>




</body>
</html>
