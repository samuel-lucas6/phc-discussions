<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style type="text/css">
body { font-size: 16px; }
.cal_brief { text-align: center; }
.cal_brief td:first-child { background: inherit; }
.cal_brief td { background: #ccc; width: 5ex; padding: 2px; }
.cal_big { text-align: center; padding: 0; margin: 0; }
.cal_big td { padding: 0 2px; }
.cal_mon { text-align: center; }
.cal_mon th { font-size: small; padding: 0; margin: 0; }
.cal_mon td { background: #ccc; width: 5ex; height: 1.5em;
	padding: 2px; text-align: right; }
.cal_mon td[colspan] { background: inherit; }
.cal_mon sup { color: #F0F0F0; text-align: left; float: left;
	margin-top: -2pt; font-weight: bold; }
.cal_mon a { text-align: right; margin-left: -4em; float: right; }
</style>

<title>phc-discussions - Re: [PHC] Compute time hardness (pwxform,blake,blamka)</title>


</head>

<BODY bgcolor="#E0E0E0" text="black" link="blue" alink="red" vlink="navy">



<TABLE bgcolor="white" width="100%" border="0" cellspacing="0" cellpadding="0">
<TR>
<TD width="39%">
<A HREF="http://lists.openwall.net">lists.openwall.net</A>
<TD width="1%" rowspan="3">&nbsp;
<TD width="60%" align="right" rowspan="3">
<A HREF="/">lists</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/announce/">announce</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-users/">owl-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-dev/">owl-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-users/">john-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-dev/">john-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwdqc-users/">passwdqc-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/yescrypt/">yescrypt</A>&nbsp;
<A HREF="http://www.openwall.com/lists/popa3d-users/">popa3d-users</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/oss-security/">oss-security</A>&nbsp;
<A HREF="http://www.openwall.com/lists/kernel-hardening/">kernel-hardening</A>&nbsp;
<A HREF="http://www.openwall.com/lists/musl/">musl</A>&nbsp;
<A HREF="http://www.openwall.com/lists/sabotage/">sabotage</A>&nbsp;
<A HREF="http://www.openwall.com/lists/tlsify/">tlsify</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwords/">passwords</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/crypt-dev/">crypt-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/xvendor/">xvendor</A>&nbsp;
/&nbsp;
<A HREF="/bugtraq/">Bugtraq</A>&nbsp;
<A HREF="/full-disclosure/">Full-Disclosure</A>&nbsp;
<A HREF="/linux-kernel/">linux-kernel</A>&nbsp;
linux-<A HREF="/netdev/">netdev</A>&nbsp;
<A HREF="/linux-ext4/">linux-ext4</A>&nbsp;
<a href="/linux-hardening/">linux-hardening</a>&nbsp;
<a href="/linux-cve-announce/">linux-cve-announce</a>&nbsp;
<a href="/phc-discussions/">PHC</a>&nbsp;
<TR><TD>
<DIV><FONT SIZE="-2"><I>Open Source and information security mailing list archives</I></FONT></DIV>
<TR><TD>&nbsp;
</TABLE>

<TABLE bgcolor="#B4D0DC" width="100%" border="0" cellspacing="0" cellpadding="1">
<TR><TD>
<TABLE width="100%" border="0" cellspacing="0" cellpadding="2">
<TR><TD bgcolor="#ECF8FF">

<a href="https://hashsuite.openwall.net/android">
Hash Suite for Android: free password hash cracker in your pocket</a>


</TABLE>
</TABLE>


<a href="../../../2015/04/06/9">[&lt;prev]</a> <a href="2">[next&gt;]</a> <a href="../../../2015/04/06/5">[&lt;thread-prev]</a> <a href="6">[thread-next&gt;]</a> <a href=".">[day]</a> <a href="..">[month]</a> <a href="../..">[year]</a> <a href="../../..">[list]</a>
<pre style="white-space: pre-wrap">
Message-ID: &lt;20150407001232.GA18442&#64;openwall.com&gt;
Date: Tue, 7 Apr 2015 03:12:32 +0300
From: Solar Designer &lt;solar&#64;...nwall.com&gt;
To: discussions&#64;...sword-hashing.net
Subject: Re: [PHC] Compute time hardness (pwxform,blake,blamka)

Hi Marcos,

On Mon, Apr 06, 2015 at 01:09:36PM -0300, Marcos Simplicio wrote:
&gt; Apologies for the delay: I had almost no Internet access this weekend.

No problem about the delay.

&gt; On 03-Apr-15 08:12, Solar Designer wrote:
&gt; &gt; There's a difference, though: I guess yescrypt's pwxform S-box lookups
&gt; &gt; are much more frequent than Lyra2's equivalent (is there one?)  
&gt; 
&gt; No, there is no table look-up or anything of the sort in any Blake
&gt; variant I'm aware of.

Of course not in Blake2, but does Lyra2 itself perform anything like it?

&gt; OTOH, what I can say is actually a question: the S-box lookups are
&gt; attack-discouraging when you look at GPUs, right? At least that is what
&gt; I grasped from previous discussions on pwxform:
&gt; 
&gt; - Multiplications are to limit speed-ups on ASICs, since CPUs have
&gt; highly-optimized hardware for this task (I'm not sure about GPUs).
&gt; 
&gt; - Look-up tables (LUTs) are GPU-unfriendly operations, at least in
&gt; theory, but are in principle very fast in hardware.
&gt; 
&gt; So far we were discussing hardware-oriented protection, so LUTs would
&gt; not count much in the comparison. Of course, I may be missing something!

You're mostly correct.  The rapid S-box lookups are primarily meant
against GPUs.  So when I mentioned them, I expanded the scope of the
comparison.

However, like MULs they are also not very fast in hardware.  In fact, in
an earlier thread here we discussed the possibility to build a 32x32-&gt;64
MUL from multiple table lookups, and the conclusion was that it's
unlikely to reduce the overall latency much if at all.  So what pwxform
achieves is ensure that a round latency is max(MUL, LUT), and it is in
fact unclear which of these is faster on attacker's platform.  This may
vary between attackers and their platforms.  It might vary even between
different reasonable ASIC designs, although chances are that LUTs (of
this size) are roughly twice faster than MULs.

&gt; BTW: we are still trying to verify the above assumptions on GPU- and
&gt; FPGA-oriented unfriendliness with actual experiments. The GPU graphs
&gt; should be ready shortly (we have a GPU specialist working on that),
&gt; while FPGAs should take a little longer (a colleague professor allocated
&gt; a student to work on that).

You've seen our results for bcrypt cracking on a variety of devices
including GPUs and FPGAs, right?

<a href="http://www.openwall.com/presentations/Passwords14-Energy-Efficient-Cracking/">http://www.openwall.com/presentations/Passwords14-Energy-Efficient-Cracking/</a>

There's also related work for bcrypt cracking on FPGAs by Friedrich
Wiemer and Ralf Zimmermann:

<a href="https://www.emsec.rub.de/media/crypto/veroeffentlichungen/2014/10/22/reconfig14_bcrypt.pdf" rel="nofollow">https://www.emsec.rub.de/media/crypto/veroeffentlichungen/2014/10/22/reconfig14_bcrypt.pdf</a>

&gt; &gt; Also, I never understood why you chose two full passes through memory as
&gt; &gt; the minimum for Lyra2.  Is this arbitrary?  For yescrypt, the 4/3 can be
&gt; &gt; shown as being the optimal stopping point assuming no AT-reducing TMTO,
&gt; &gt; so it's not arbitrary at all.
&gt; 
&gt; OK, that is our fault: that should be clearer on our documentation...
&gt; The rationale is a little long, so please bear with me.

I found this very interesting, thank you!

&gt; In regular executions with large amounts of memory, memory latency and
&gt; bandwidth are expected to (and, experimentally, they do) play an
&gt; important role. The result is that using much faster computation
&gt; hardware in attacks may end up being of little use: it will not
&gt; accelerate the attack by much, just like increasing the number of rounds
&gt; 10 times in a legitimate platform does not slow down the execution 10
&gt; times either...

Right.  Computation speedup might or might not provide a comparable
speedup of attacks.

&gt; Now, attackers could combine faster computation hardware with faster
&gt; memory technology, thus taking full advantage of the former to
&gt; accelerate attacks, in which case computation hardening indeed becomes
&gt; an important requirement. However, using fast memory technology for a
&gt; PHS with a high memory usage (e.g., 1GB of high-speed registers) is
&gt; probably unrealistic, as it would become very expensive to test a single
&gt; password, let alone several passwords in parallel. It would probably be
&gt; more reasonable to buy more (less expensive) hardware and get a higher
&gt; password testing throughput simply with parallel tests.

This is unclear.  Faster memory technology is already being developed,
and it won't necessarily be prohibitively expensive.  The question is
whether it'd become equally available on typical defenders' hardware and
on attackers' most cost-effective specialized/chosen hardware, or not.

&gt; On the other hand, using fast memory is very reasonable for accelerating
&gt; operations that involve only a small amount of memory. This is exactly
&gt; what a legitimate user's cache is for, so maximizing cache usage on the
&gt; legitimate platform is (at least from this perspective) a good approach:
&gt; attackers should have at least the same amount of cache to get the same
&gt; speed-ups, while having much more cache only helps if the data to be
&gt; processed is in that part of memory (e.g., if the required data can be
&gt; prefetched, something reasonable in deterministic, cache-timing
&gt; resistant schemes/passes, or, again, if the whole memory is a huge cache).

Yes.  So yescrypt's S-boxes tuned for current CPUs' L1 cache per thread
favor current defenders vs. possible attackers' hardware mostly other
than ASICs (some kilobytes of SRAM in an ASIC is almost negligible, if
we also use megabytes of RAM).

&gt; There is, however, another situation that involves only small amounts of
&gt; memory and, thus, in which attackers can benefit from faster memory
&gt; technology: recomputations during a TMTO attack. After all, whether the
&gt; scheme/pass is deterministic or password dependent, recomputations
&gt; become password-independent as long as the attacker stores the sequence
&gt; of rows required after seeing them in original computation; those rows
&gt; can, thus, be prefetched and stored in cache-like memory. As a result,
&gt; while the original computation of a block V_i (discarded) from a set of
&gt; blocks \phi{V_i} took, say, 20\sigma (where \sigma accounts both for
&gt; memory- and processing-related costs), a hardware acceleration of 10x
&gt; combined with a 10x faster memory could bring the cost of recomputations
&gt; to approximately 1 if \sigma is 50% memory and 50% processing;
&gt; parallelism and pipelining, if possible, may allow recomputations to
&gt; occur even faster. The lesson here is that, even though at first sight
&gt; the time*memory cost would appear to the around 20, in practice it might
&gt; be much lower than that. That is why computing the actual time*memory
&gt; cost is tricky, especially without experiments to verify them, and why I
&gt; believe TMTO resistance should involve a reasonable security margin
&gt; (something that Argon and Lyra2, for example, try to achieve with their
&gt; minimum settings).

Basically, you're saying that TMTO reduces not only the memory
component, but also a time component - specifically, memory latency - so
the theoretical memory*time calculations that assume same time per round
of a primitive, etc. do not reflect the reality.  OK, this is finally a
well-reasoned argument in favor of a TMTO security margin.  (It's also
an argument in favor of compute time hardening, which makes having a
TMTO security margin less important.)

I actually considered this aspect in yescrypt's support for thread-level
parallelism, where it tries to discourage the obvious TMTO that is
otherwise possible with sequential computation of the would-be-threads'
workload.  This TMTO attack would not reduce the theoretical and naive
memory*time product, but it may reduce real life's due to lower memory
latency.

---
The YESCRYPT_RW flag moves this parallelism to a slightly lower level,
inside SMix.  This reduces flexibility for efficient computation (for
both attackers and defenders) by requiring that, short of resorting to a
TMTO attack on ROMix, the full amount of memory be allocated as needed
for the specified p, regardless of whether that parallelism is actually
being fully made use of or not.  This may be desirable when the defender
has enough memory with sufficiently low latency and high bandwidth for
efficient full parallel execution, yet the required memory size is high
enough that some likely attackers might end up being forced to choose
between using higher latency memory than they could use otherwise
(waiting for data longer) or using TMTO (waiting for data more times per
one hash computation).  The area-time cost for other kinds of attackers
(who would use the same memory type and TMTO factor or no TMTO either
way) remains roughly the same, given the same running time for the
defender.
---

&gt; This desire for a security margin is what motivated Lyra2's t_min=1,
&gt; which can be considered "arbitrary" but not "random": it was based on
&gt; the security analysis we tried to provide together with the algorithm's
&gt; documentation. Based on what we have seen as effective attacks, I
&gt; believe that (and, again, that is open to debate!):
[...]
&gt; Without further experimentation, however, I do not feel comfortable
&gt; speculating further on potential drawbacks of each approach, though. I
&gt; can only say that we do pay a price in terms of performance trying to
&gt; provide some cache-timing resistance, while it might be better to go
&gt; with Argon2's approach of separating things in "d" and "i" (explaining
&gt; the difference to users, however, may not be very easy).

Thanks for explaining your rationale.  All of this (including the
portion I skipped from quoting) makes sense to me.

&gt; Anyhow, I do not think t_min is critical for any scheme during the
&gt; competition itself: a better understanding on attacks may motivate
&gt; authors to set it higher or lower, and the final decision of what should
&gt; be the minimum is probably something that needs to be discussed and
&gt; sanctioned by the PHC panel together with the authors and the community
&gt; in general. SHA-3's capacity is an example of such after-competition
&gt; effort: although it is probably a bad example due to the debatable
&gt; choices NIST tried to push, the idea of "fixing some knobs" after a
&gt; scheme was selected is not bad per se.

I agree.  Unfortunately, we don't currently have another tweaking phase
planned, so we have to pay attention to the current settings as well.

&gt; And that is also why comparisons
&gt; with a similar number of passes through memory are useful, even if not
&gt; definitive to say which scheme is "the best".

While I agree that t_min isn't critical, you've provided some good
reasons why Lyra2's isn't currently set lower.  Most notably, its
partial cache-timing resistance.  With this, I think it primarily makes
sense to compare yescrypt vs. Lyra2 at their current t_min's, but also
to take into consideration that Lyra2 buys us partial cache-timing
resistance, whereas yescrypt (mostly) does not.

&gt; &gt; I currently recommend that yescrypt be used with t=0 unless more time
&gt; &gt; can be spent and more memory can't be in a given case.
&gt; 
&gt; That is reasonable. I'm just not sure what is the "security margin" in
&gt; practice (again, because computing time*memory cost is
&gt; technology-dependent). Just to avoid misunderstandings: no FUD intended
&gt; here! Unless someone can come up with an effective attack, there is no
&gt; reason to think yescrypt's assumptions are wrong, so t=0 may be
&gt; perfectly safe.

I expect that there may in fact be cases where a low TMTO factor like
1/2 would turn out to be optimal when attacking yescrypt t=0.  However,
I expect those to be relatively rare special cases, and I expect that
the alternative of having yescrypt run to higher t would have been worse
even in most of those special cases (it'd defeat TMTO, but the overall
memory usage would be less without TMTO).

I cared about those TMTOs that don't reduce theoretical memory*time more
for the thread-level parallelism, because potentially much higher TMTO
factors would otherwise be practical there: proportional to the number
of threads, which for some use cases might be very high.  I think the
current yescrypt design greatly reduces the TMTO factors that would
actually turn out to be optimal in those cases (and requires TMTO to be
applied at ROMix level rather than at thread level).

&gt; &gt;&gt; - For 1 thread ("p1") with different passes through memory: say,
&gt; &gt;&gt; yescrypt with T=0 does 4/3 passes though memory with 12 MUL+ADD+XOR,
&gt; &gt;&gt; while Lyra2 with T=1 and BlaMka makes 2 passes through memory with 8
&gt; &gt;&gt; MUL+ADD+XOR are comparable (?) in terms of compute-time latency (4/3*12
&gt; &gt;&gt; = 2*8 = 16). Lyra2 in this case is slightly (3%) slower. Given the low
&gt; &gt;&gt; difference, the reason is probably related to bandwidth usage, because
&gt; &gt;&gt; Lyra2 purposely uses more bandwidth in this scenario: namely, it makes 2
&gt; &gt;&gt; writes per call of the underlying hash instead of 1, and 4 reads (2 from
&gt; &gt;&gt; cache and 2 from memory) instead of 2. I'm not sure if that is good or
&gt; &gt;&gt; bad, but improvements on memory bandwidth are usually slower than on
&gt; &gt;&gt; computation power, so I would rather say it is good in terms of attack
&gt; &gt;&gt; resistance, as it limits an attacker's parallelism ability.
&gt; &gt; 
&gt; &gt; This makes sense, but you also need to factor in yescrypt's S-box lookups.
&gt; 
&gt; I agree this factor is likely to play a role. However, Bill reported a
&gt; long time ago speed-ups of ~20% if Lyra2 reduced its number of memory
&gt; writes when no parallelism is employed. We preferred not do so in the
&gt; 1-threaded version, however, because we _wanted_ attackers to run into
&gt; memory bounds as soon as possible (again, because users and attackers
&gt; are likely to have access to the same memory technology).

I didn't mean that yescrypt's S-box lookups slow it down a lot - I think
they slow it down just a bit.  This is a measurable slowdown, though.
(I designed and re-designed pwxform until the performance impact of
S-box lookups was low, while their frequency was bcrypt-like.)

I primarily meant that they add to defense too, so same speed of Lyra2
and yescrypt on a given benchmark may actually mean that yescrypt wins
in terms of the compute hardening against various platforms.

&gt; Nevertheless, that discussion and further tests motivated a bandwidth
&gt; reduction for higher p,

While you can tweak things based on p for KDF use, you can't do it for
password hashing use - you just don't know how many threads are being
run simultaneously (external to your code).

&gt; which justifies the better results for p &gt; 1:
&gt; the extra cores provide both the higher bandwidth usage we wanted and
&gt; similar TMTO security margins. The side effects are, however:
&gt; 
&gt; 1) It does become memory bound with degrees of parallelism higher than ~4
&gt; 
&gt; 2) More parallelism allows better performance of attacks using GPUs
&gt; (e.g., the highest throughput of GPU-based attacks reported in the
&gt; Reference Guide is for p = 4, the maximum p we tested)

Of course, more/excessive parallelism favors attackers.

&gt; All in all, it is quite possible that Lyra2 is not as good at using the
&gt; whole budget of processing cores as schemes with a lower bandwidth
&gt; usage, unless further tweaking is done (outside the competition, of course).

Right.

&gt; &gt; What machine is this, for these benchmarks?
&gt; 
&gt; Sorry: I forgot to mention... It is a Intel Xeon
&gt; E5-2430, with 48 GB of DRAM, running Ubuntu 14.04
&gt; LTS 64 bits
&gt; 
&gt; (Note: this is the same computer used for the benchmarks in our
&gt; reference guide. The number of physical cores is 6, however, not 12 as
&gt; reported there: 12 is the number of threads)

Yes, and I recall asking you for p=12 benchmarks before. ;-)  Thank you
for providing them now.

&gt; &gt; Would you also benchmark Lyra2 vs. yescrypt for the maximum number of
&gt; &gt; threads supported in hardware on your machine, please?
&gt; 
&gt; Sure! Attached are the graphs for 1-14 threads (more than 12, just to
&gt; see what happens).

Thank you!

For more than 12, what happens is implementation and system rather than
design specific.  It also matters whether this is merely p=14 or actual
14 threads running concurrently.

For yescrypt, a (minor) goal was that there should be no (significant)
reduction in requests/second throughput with growing number of
concurrent independent threads (as with password hashing on a server),
including beyond the server's supported hardware threads (so with
context switching).  I ran some experiments of this sort.  This is also
a minor reason why PWXrounds is currently set so high.

With p=14 in one invocation, you may in fact see a slowdown, because by
default OpenMP does not run more threads than your system supports in
hardware.  The two extra threads' workloads were waiting for two threads
to be done with their initial workloads.  With p=24, you'd see optimal
behavior again.

&gt; As mentioned above, Lyra2 gets memory bound after ~4 cores, so there is
&gt; little difference in performance between Blake2 or BlaMka with a
&gt; different number of rounds.
&gt; 
&gt; Yescrypt appears to scale better with more cores, apparently because it
&gt; does not get (much) memory-bound. We notice that the performance gains
&gt; seem to be more expressive up to the point the maximum number of
&gt; physical cores is reached, although it may be a coincidence.

This is not a coincidence.  yescrypt with its current pwxform settings
contains almost enough instruction level parallelism to fully load your
CPU's cores even without running 2 threads/core.  This was a tough
decision (on tuning the default settings), because excessive parallelism
helps attacks.  I made it this way in part because some recent/cheaper
Intel CPUs come with HT disabled (and some people disable HT in BIOS
settings) and in part because this extra parallelism will be made use of
with AVX2 code (so 2 threads/core would be required there anyway).

Another aspect is that you might have needed to set

export GOMP_CPU_AFFINITY=0-11

to have OpenMP work more optimally for &gt; 6 threads on your machine.

&gt; One question here is: does yescrypt design tries to make attackers
&gt; memory-bound too? I mean, it clearly takes better advantage of the
&gt; "processing cores" budget, but cores are something that attackers are
&gt; likely to have access more easily than defenders, while "better memory
&gt; technology" not so much. This logic may be completely wrong (especially
&gt; without experimental results to back it up), but, for better or for
&gt; worse, that is something we considered when designing Lyra2.

yescrypt tries to maximize use of multiple resources at once, so it
tries not to bump into memory bandwidth before reaching the maximum
number of threads supported in hardware.

I was primarily optimizing it for password hashing on a server, where
the requests rate capacity (and thus what cost settings the sysadmins
would choose for new passwords) is determined by what happens at full
load - so with the maximum number of concurrent instances running.

This makes it suboptimal for KDF use at low p.  Unfortunately, low p
does not indicate there are not lots of concurrent independent threads
running, which is why I didn't opt to tune PWXrounds based on p.  Maybe
there should be an implementation-level flag to tune for KDF use at low
p (would in fact reduce PWXrounds if so).

&gt; &gt; Are both Lyra2 and yescrypt
&gt; &gt; built for the same SIMD intrinsics (and which)?
&gt; 
&gt; Not really: Lyra2 uses SSE2 ("-msse2" flag), while yescrypt was built
&gt; with the default makefile, with its "-march=native" flag. So, in the
&gt; words of my student who run the benchmarks, "yescrypt is running SSE4.1
&gt; and/or AVX".
&gt; 
&gt; I will have to check in more detail to avoid saying anything wrong, but
&gt; it appears we are running yescrypt with more help from SIMD instructions
&gt; than Lyra2...

It sounds so.

&gt; BTW, we have not yet used the suggestion by Samuel Neves regarding NORX
&gt; tricks ("We used this trick (replacing + with ^, * with &amp;) in NORX (pg.
&gt; 23 of <a href="https://norx.io/data/norx.pdf" rel="nofollow">https://norx.io/data/norx.pdf</a>); it does noticeably
&gt; improve latency."). We will try and see what happens.

I think Samuel was referring to the operation re-ordering trick I had
mentioned, not to "replacing + with ^, * with &amp;".  Replacing * with &amp;
would obviously remove the multiply latency hardening.

&gt; &gt; There's significant room for improvement for yescrypt with AVX2, which I
&gt; &gt; haven't added yet.  In fact, on pre-AVX2 CPUs the compute hardening can
&gt; &gt; be doubled by reducing the parallelism from 512-bit to 256-bit, with
&gt; &gt; only a moderate slowdown, so the hardening per defensive running time
&gt; &gt; would likely be improved by 50% or so - but I preferred the defaults to
&gt; &gt; be balanced, with newer and near future CPUs in consideration as well,
&gt; &gt; and also with CPU attacks considered (where an attacker could bring the
&gt; &gt; extra parallelism in from having multiple candidate passwords to test).
&gt; 
&gt; That sounds reasonable. One caveat, though, is that if we really are
&gt; going to take into account more limited platforms (e.g., 8- or 16-bit
&gt; microcontrollers) as a possible use case, then fixing the underlying
&gt; hash to pwxform may not be the best approach (possibly a "pwxformLite"
&gt; would be necessary).

Yes, I had thought of that, but I listened to the criticism (and my own
opinion) of yescrypt being too complex already.  Currently, pwxform
scales down to one 64-bit lane, but not below that.  This means it has
2x maybe-excessive parallelism when running on some 32-bit CPUs.  As to
microcontrollers, those would need a MUL-less version of pwxform.  My
idea was to provide a 32-bit MUL-less version like that (similar to or
even exactly a Blowfish round) when PWXgather = PWXsimple = 0.  Maybe
later, if there's demand.  Right now, yescrypt's native mode is not for
systems this small.

Wow.  This reply took time to write.  Maybe we should avoid lengthy
messages like that.

Thanks again,

Alexander
</pre>
<p><a href="https://www.openwall.com/blists/">Powered by blists</a> - <a href="https://lists.openwall.net">more mailing lists</a>


<p>




</body>
</html>
