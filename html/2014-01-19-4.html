<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style type="text/css">
body { font-size: 16px; }
.cal_brief { text-align: center; }
.cal_brief td:first-child { background: inherit; }
.cal_brief td { background: #ccc; width: 5ex; padding: 2px; }
.cal_big { text-align: center; padding: 0; margin: 0; }
.cal_big td { padding: 0 2px; }
.cal_mon { text-align: center; }
.cal_mon th { font-size: small; padding: 0; margin: 0; }
.cal_mon td { background: #ccc; width: 5ex; height: 1.5em;
	padding: 2px; text-align: right; }
.cal_mon td[colspan] { background: inherit; }
.cal_mon sup { color: #F0F0F0; text-align: left; float: left;
	margin-top: -2pt; font-weight: bold; }
.cal_mon a { text-align: right; margin-left: -4em; float: right; }
</style>

<title>phc-discussions - Re: [PHC] Question about saturating the memory bandwidth</title>


</head>

<BODY bgcolor="#E0E0E0" text="black" link="blue" alink="red" vlink="navy">



<TABLE bgcolor="white" width="100%" border="0" cellspacing="0" cellpadding="0">
<TR>
<TD width="39%">
<A HREF="http://lists.openwall.net">lists.openwall.net</A>
<TD width="1%" rowspan="3">&nbsp;
<TD width="60%" align="right" rowspan="3">
<A HREF="/">lists</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/announce/">announce</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-users/">owl-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-dev/">owl-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-users/">john-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-dev/">john-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwdqc-users/">passwdqc-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/yescrypt/">yescrypt</A>&nbsp;
<A HREF="http://www.openwall.com/lists/popa3d-users/">popa3d-users</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/oss-security/">oss-security</A>&nbsp;
<A HREF="http://www.openwall.com/lists/kernel-hardening/">kernel-hardening</A>&nbsp;
<A HREF="http://www.openwall.com/lists/musl/">musl</A>&nbsp;
<A HREF="http://www.openwall.com/lists/sabotage/">sabotage</A>&nbsp;
<A HREF="http://www.openwall.com/lists/tlsify/">tlsify</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwords/">passwords</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/crypt-dev/">crypt-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/xvendor/">xvendor</A>&nbsp;
/&nbsp;
<A HREF="/bugtraq/">Bugtraq</A>&nbsp;
<A HREF="/full-disclosure/">Full-Disclosure</A>&nbsp;
<A HREF="/linux-kernel/">linux-kernel</A>&nbsp;
linux-<A HREF="/netdev/">netdev</A>&nbsp;
<A HREF="/linux-ext4/">linux-ext4</A>&nbsp;
<a href="/linux-hardening/">linux-hardening</a>&nbsp;
<a href="/linux-cve-announce/">linux-cve-announce</a>&nbsp;
<a href="/phc-discussions/">PHC</a>&nbsp;
<TR><TD>
<DIV><FONT SIZE="-2"><I>Open Source and information security mailing list archives</I></FONT></DIV>
<TR><TD>&nbsp;
</TABLE>

<TABLE bgcolor="#B4D0DC" width="100%" border="0" cellspacing="0" cellpadding="1">
<TR><TD>
<TABLE width="100%" border="0" cellspacing="0" cellpadding="2">
<TR><TD bgcolor="#ECF8FF">

<a href="https://hashsuite.openwall.net/android">
Hash Suite for Android: free password hash cracker in your pocket</a>


</TABLE>
</TABLE>


<a href="3">[&lt;prev]</a> <a href="5">[next&gt;]</a> <a href="3">[&lt;thread-prev]</a> <a href=".">[day]</a> <a href="..">[month]</a> <a href="../..">[year]</a> <a href="../../..">[list]</a>
<pre style="white-space: pre-wrap">
Message-ID: &lt;20140119100713.GA18640&#64;openwall.com&gt;
Date: Sun, 19 Jan 2014 14:07:13 +0400
From: Solar Designer &lt;solar&#64;...nwall.com&gt;
To: discussions&#64;...sword-hashing.net
Subject: Re: [PHC] Question about saturating the memory bandwidth

On Sun, Jan 19, 2014 at 01:13:33PM +0400, Solar Designer wrote:
&gt; With Salsa20/2, I am getting 9300 c/s:
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 845 c/s real, 852 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 9300 c/s real, 292 c/s virtual

Without ROM (all reads are from RAM), also 1.75 MiB RAM:

Benchmarking 1 thread ...
1248 c/s real, 1255 c/s virtual
Benchmarking 32 threads ...
15429 c/s real, 484 c/s virtual

15429*1.75*4*2^30/10^9 = ~116 GB/s

L3 cache helps much more here (we have 1.75*32 = 56 MiB of data).

&gt; or 4188 c/s with 3.5 MiB RAM:
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 429 c/s real, 429 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 4188 c/s real, 132 c/s virtual

Benchmarking 1 thread ...
635 c/s real, 639 c/s virtual
Benchmarking 32 threads ...
4774 c/s real, 150 c/s virtual

4774*3.5*4*2^30/10^9 = ~71.8 GB/s

L3 cache is not of as much help, as we're exceeding its size by more and
also because:

&gt; 7 MiB:
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 214 c/s real, 212 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 1992 c/s real, 62 c/s virtual

Without ROM:

Benchmarking 1 thread ...
321 c/s real, 323 c/s virtual
Benchmarking 32 threads ...
1987 c/s real, 62 c/s virtual

Faster for 1 thread, but almost same speed for 32 threads.

&gt; 14 MiB:
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 101 c/s real, 101 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 952 c/s real, 30 c/s virtual

Benchmarking 1 thread ...
160 c/s real, 160 c/s virtual
Benchmarking 32 threads ...
892 c/s real, 28 c/s virtual

Now it's even slower for 32 threads.  Why?  I think it's page size.  We
had the ROM in SysV shm on 2 MB pages (explicitly requested), whereas
the RAM is allocated with mmap() without an explicit page size request.
I guess the kernel kept it on 4 KB pages here.  Thus, re-pointing half
the reads from ROM on 2 MB pages to RAM on 4 KB pages may slow them
down, given that the data does not fit in L3 cache (by far) anyway.

This means that when we're using a few MB or more as RAM, we can already
benefit from 2 MB pages.

&gt; To fit in L3 cache, here's 896 KiB (7/8 of a MiB):
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 1637 c/s real, 1650 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 22250 c/s real, 701 c/s virtual
&gt; 
&gt; 22250*7/8*4*2^30/10^9 = ~83.6 GB/s
&gt; 
&gt; This uses 28 MiB out of 32 MiB L3 cache, but there's possibly some cache
&gt; thrashing by the reads from ROM (even though they're non-temporal, with
&gt; the hint).  Let's try 448 KiB (7/16 of a MiB):
&gt; 
&gt; Benchmarking 1 thread ...
&gt; 3102 c/s real, 3102 c/s virtual
&gt; Benchmarking 32 threads ...
&gt; 57406 c/s real, 1803 c/s virtual
&gt; 
&gt; 57406*7/16*4*2^30/10^9 = ~107.9 GB/s

Same as above, but without ROM:

Benchmarking 1 thread ...
2380 c/s real, 2408 c/s virtual
Benchmarking 32 threads ...
46534 c/s real, 1462 c/s virtual

46534*7/8*4*2^30/10^9 = ~175 GB/s

Benchmarking 1 thread ...
4403 c/s real, 4427 c/s virtual
Benchmarking 32 threads ...
83581 c/s real, 2628 c/s virtual

83581*7/16*4*2^30/10^9 = ~157 GB/s

The efficiency loss with smaller memory size is puzzling.  At these high
speeds, some kind of overhead probably starts to play more of a role.

Finally, with 1 MiB per instance to exactly match L3 cache size (r=8, so
1 KiB blocks):

Benchmarking 1 thread ...
2078 c/s real, 2089 c/s virtual
Benchmarking 32 threads ...
40678 c/s real, 1275 c/s virtual

40678*4*2^30/10^9 = ~175 GB/s

Oh, and with only 1 round of Salsa20:

Benchmarking 1 thread ...
3078 c/s real, 3102 c/s virtual
Benchmarking 32 threads ...
55840 c/s real, 1742 c/s virtual

55840*4*2^30/10^9 = ~240 GB/s

Somehow r=4 is faster in this case (same total allocation size):

Benchmarking 1 thread ...
3125 c/s real, 3150 c/s virtual
Benchmarking 32 threads ...
59062 c/s real, 1846 c/s virtual

59062*4*2^30/10^9 = ~254 GB/s

To remind, for sequential reads from L3 cache (with no processing at
all) I am getting ~400 GB/s.

In all of the above tests, the 32 threads are independent.  They're not
hashing the same password, unlike in my tests with a 2 GiB RAM
allocation that I had posted before.  Those other tests were for KDF
use (to be directly comparable to Bill's).  These high c/s rate tests
are for password hashing use, hence I expect the parallelism to come
from concurrent authentication attempts.

The "*4" factor can be reduced to "*3" or "*2" (with different escrypt
flags), resulting in higher memory usage for same c/s rate, but with
lower or with no TMTO resistance.

Alexander
</pre>
<p><a href="https://www.openwall.com/blists/">Powered by blists</a> - <a href="https://lists.openwall.net">more mailing lists</a>


<p>




</body>
</html>
