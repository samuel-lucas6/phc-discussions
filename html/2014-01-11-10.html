<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style type="text/css">
body { font-size: 16px; }
.cal_brief { text-align: center; }
.cal_brief td:first-child { background: inherit; }
.cal_brief td { background: #ccc; width: 5ex; padding: 2px; }
.cal_big { text-align: center; padding: 0; margin: 0; }
.cal_big td { padding: 0 2px; }
.cal_mon { text-align: center; }
.cal_mon th { font-size: small; padding: 0; margin: 0; }
.cal_mon td { background: #ccc; width: 5ex; height: 1.5em;
	padding: 2px; text-align: right; }
.cal_mon td[colspan] { background: inherit; }
.cal_mon sup { color: #F0F0F0; text-align: left; float: left;
	margin-top: -2pt; font-weight: bold; }
.cal_mon a { text-align: right; margin-left: -4em; float: right; }
</style>

<title>phc-discussions - Re: [PHC] escrypt memory access speed (Re: [PHC] Reworked KDF available on github for feedback: NOELKDF)</title>


</head>

<BODY bgcolor="#E0E0E0" text="black" link="blue" alink="red" vlink="navy">



<TABLE bgcolor="white" width="100%" border="0" cellspacing="0" cellpadding="0">
<TR>
<TD width="39%">
<A HREF="http://lists.openwall.net">lists.openwall.net</A>
<TD width="1%" rowspan="3">&nbsp;
<TD width="60%" align="right" rowspan="3">
<A HREF="/">lists</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/announce/">announce</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-users/">owl-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/owl-dev/">owl-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-users/">john-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/john-dev/">john-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwdqc-users/">passwdqc-users</A>&nbsp;
<A HREF="http://www.openwall.com/lists/yescrypt/">yescrypt</A>&nbsp;
<A HREF="http://www.openwall.com/lists/popa3d-users/">popa3d-users</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/oss-security/">oss-security</A>&nbsp;
<A HREF="http://www.openwall.com/lists/kernel-hardening/">kernel-hardening</A>&nbsp;
<A HREF="http://www.openwall.com/lists/musl/">musl</A>&nbsp;
<A HREF="http://www.openwall.com/lists/sabotage/">sabotage</A>&nbsp;
<A HREF="http://www.openwall.com/lists/tlsify/">tlsify</A>&nbsp;
<A HREF="http://www.openwall.com/lists/passwords/">passwords</A>&nbsp;
/&nbsp;
<A HREF="http://www.openwall.com/lists/crypt-dev/">crypt-dev</A>&nbsp;
<A HREF="http://www.openwall.com/lists/xvendor/">xvendor</A>&nbsp;
/&nbsp;
<A HREF="/bugtraq/">Bugtraq</A>&nbsp;
<A HREF="/full-disclosure/">Full-Disclosure</A>&nbsp;
<A HREF="/linux-kernel/">linux-kernel</A>&nbsp;
linux-<A HREF="/netdev/">netdev</A>&nbsp;
<A HREF="/linux-ext4/">linux-ext4</A>&nbsp;
<a href="/linux-hardening/">linux-hardening</a>&nbsp;
<a href="/linux-cve-announce/">linux-cve-announce</a>&nbsp;
<a href="/phc-discussions/">PHC</a>&nbsp;
<TR><TD>
<DIV><FONT SIZE="-2"><I>Open Source and information security mailing list archives</I></FONT></DIV>
<TR><TD>&nbsp;
</TABLE>

<TABLE bgcolor="#B4D0DC" width="100%" border="0" cellspacing="0" cellpadding="1">
<TR><TD>
<TABLE width="100%" border="0" cellspacing="0" cellpadding="2">
<TR><TD bgcolor="#ECF8FF">

<a href="https://hashsuite.openwall.net/android">
Hash Suite for Android: free password hash cracker in your pocket</a>


</TABLE>
</TABLE>


<a href="9">[&lt;prev]</a> <a href="11">[next&gt;]</a> <a href="9">[&lt;thread-prev]</a> <a href="11">[thread-next&gt;]</a> <a href=".">[day]</a> <a href="..">[month]</a> <a href="../..">[year]</a> <a href="../../..">[list]</a>
<pre style="white-space: pre-wrap">
Message-ID: &lt;20140111192317.GA7038&#64;openwall.com&gt;
Date: Sat, 11 Jan 2014 23:23:17 +0400
From: Solar Designer &lt;solar&#64;...nwall.com&gt;
To: discussions&#64;...sword-hashing.net
Subject: Re: [PHC] escrypt memory access speed (Re: [PHC] Reworked KDF available on github for feedback: NOELKDF)

Bill,

On Sat, Jan 11, 2014 at 10:50:53PM +0400, Solar Designer wrote:
&gt; I don't get why limiting the number of threads to what the code can
&gt; actually use (with its p=8 setting) reduces performance on this test,

Got it: I totally forgot that I had another 32-thread job running on the
machine, at the lowest priority (nice +19).  So when I ran only 8 new
threads, they shared cores with that other job, whereas when I ran all
32 new threads, they almost fully replaced the other job (apparently,
OpenMP's spinning threads were taking up less resources than that other
job's actual computation threads).

With that other job stopped, I get the following numbers for 1 Salsa20
round, p=8, and only 8 running threads:

real    0m1.483s
user    0m11.398s
sys     0m0.383s

2*3*10*2^30/10^9/1.483 = 43.44 GB/s

Ditto, 2 rounds:

real    0m1.723s
user    0m13.331s
sys     0m0.370s

2*3*10*2^30/10^9/1.723 = 37.39 GB/s

4 rounds:

real    0m2.485s
user    0m19.404s
sys     0m0.383s

2*3*10*2^30/10^9/2.485 = 25.93 GB/s

8 rounds:

real    0m4.256s
user    0m33.547s
sys     0m0.367s

2*3*10*2^30/10^9/4.293 = 15.01 GB/s

So it appears that on the idle machine (as it should have been), 8
threads are almost enough to use the 8 memory channels, but not enough
to reach similar GB/s speeds when Salsa20 rounds count is increased.

&gt; Roughly same speed as for p=8.  That's good.  But now we can go for 32
&gt; threads for real:
&gt; 
&gt; real    0m1.293s
&gt; user    0m39.497s
&gt; sys     0m1.141s

New speed for 1 Salsa20 round and p=32, on idle machine:

real    0m1.286s
user    0m39.523s
sys     0m1.183s

Not much change here. :-)

2*3*10*2^30/10^9/1.286 = 50.10 GB/s

&gt; Back to 2 rounds of Salsa20:
&gt; 
&gt; real    0m1.316s
&gt; user    0m39.959s
&gt; sys     0m1.353s
&gt; 
&gt; 2*3*10*2^30/10^9/1.316 = 48.95 GB/s

real    0m1.305s
user    0m40.109s
sys     0m1.242s

49.37 GB/s

&gt; 4 rounds:
&gt; 
&gt; real    0m1.412s
&gt; user    0m43.291s
&gt; sys     0m1.130s
&gt; 
&gt; 2*3*10*2^30/10^9/1.412 = 45.63 GB/s

real    0m1.404s
user    0m43.245s
sys     0m1.229s

45.89 GB/s

&gt; 8 rounds:
&gt; 
&gt; real    0m1.694s
&gt; user    0m52.293s
&gt; sys     0m1.068s
&gt; 
&gt; 2*3*10*2^30/10^9/1.694 = 38.03 GB/s

real    0m1.684s
user    0m52.329s
sys     0m1.109s

38.26 GB/s

So the same conclusions still apply:

&gt; If we stay with pure Salsa20, then maybe 8, 4, or 2 rounds is optimal.
&gt; There's only a 2% speedup from going further to 1 round.
&gt; 
&gt; In fact, if we were to regard only the Salsa20 rounds as computation
&gt; that the attacker will need to perform, then even round counts higher
&gt; than 8 could be more optimal in attack area*time terms, from defender's
&gt; point of view.  Since GB/s is proportional to memory usage for fixed
&gt; running time (or actually the other way around), we can estimate AT
&gt; costs (in arbitrary units) as follows:
&gt; 
&gt; 48.95 GB/s * 2 rounds = 97.90
&gt; 45.63 GB/s * 4 rounds = 182.52
&gt; 38.03 GB/s * 8 rounds = 304.24
&gt; 30.43 GB/s * 12 rounds = 365.16
&gt; 19.93 GB/s * 20 rounds = 398.60

... but these are with all cores in use.  With only 8 threads running,
lower Salsa20 round counts are more beneficial.

&gt; However, in practice the "overhead" XORs and ADDs are probably no less
&gt; costly than those that are part of Salsa20 rounds, so 4 or 8 rounds may
&gt; be good.  2 rounds does feel potentially worse in terms of significantly
&gt; reduced computation (and thus potentially similarly reduced "time"
&gt; factor for the attacker with a more suitable hardware architecture).

Alexander
</pre>
<p><a href="https://www.openwall.com/blists/">Powered by blists</a> - <a href="https://lists.openwall.net">more mailing lists</a>


<p>




</body>
</html>
